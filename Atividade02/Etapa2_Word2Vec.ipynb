{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 2 - Etapa 2 - Tokenização com Word2Vec\n",
    "\n",
    "Tokenização do dataset de notícias(Folha UOL) utilizando o modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação do dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/articles.csv', encoding='utf8')\n",
    "print('linhas, colunas):', df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ajustes para aprendizado - já realizados na Etapa1_v02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "df.drop(['subcategory', 'link', 'date'], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utilizarei as 7 categorias mais relevantes no treinamento a fim de deixar o dataset mais balanceado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category'].isin(list(df['category'].value_counts()[:7].index))]\n",
    "\n",
    "df['category'] = df['category'].replace('guia-de-livros-discos-filmes', 'guia-de-livros-filmes-discos')\n",
    "alvo, valores = pd.factorize(df['category'], sort=True)\n",
    "df['target'] = alvo\n",
    "print('Novo tamanho do dataset:', len(df))\n",
    "list(enumerate(valores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#instalação do modelo 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remoção de Palavras Irrelevantes (Stopwords)**\n",
    "\n",
    "Não acrescentam muito a sentença e podem ser facilmente ignoradas sem sacrificar o significado do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('portuguese'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('', '', text)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    #text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "#Apply function on review column\n",
    "df['text']=df['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORDCLOUD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criação de uma nuvem de palavras para textos da categoria 'poder'\n",
    "plt.figure(figsize = (10, 10)) # Text that is Not Sarcastic\n",
    "wc = WordCloud(max_words=2000 , width=1600 , height=800).generate(\" \".join(df[df.category == 'poder'].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introdução à incorporação de palavras e Word2Vec**\n",
    "\n",
    "Representação do vocabulário de documentos. É capaz de de capturar o contexto de uma palavra em um documento, semelhança semântica e sintática, relação com outras palavras.\n",
    "\n",
    "Incorporação de palavras: representações vetoriais de uma palavra específica. Word2Vec é uma técnica para aprender incorporações de palavras usando rede neural superficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertendo para um formato aceitado pela biblioteca _gensim_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in df.text.values:\n",
    "    # se não for string, converte para string\n",
    "    if not isinstance(i, str):\n",
    "        i = str(i)\n",
    "    words.append(i.split())\n",
    "\n",
    "words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# dimensão do vetor a ser gerado\n",
    "EMBEDDING_DIM = 100\n",
    "# criando um vetor de palavras através do Método Word2Vec \n",
    "w2v_model = gensim.models.Word2Vec(sentences=words, vector_size=EMBEDDING_DIM, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=35000)\n",
    "tokenizer.fit_on_texts(words)\n",
    "tokenized_train = tokenizer.texts_to_sequences(words)\n",
    "x = sequence.pad_sequences(tokenized_train , maxlen=20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
